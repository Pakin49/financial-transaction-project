{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4vYMmiR7Kbc"
      },
      "source": [
        "# Analyzing Financial Transaction Data using Spark âœ¨\n",
        "## Topics\n",
        "- Use SparkSQL for database\n",
        "- Regression\n",
        "- Classification\n",
        "- Clustering\n",
        "- Summarize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov1EBLcy_lOb"
      },
      "source": [
        "# Initialization (Must do)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV_p4FSK7CW_",
        "outputId": "0b1df768-d216-4cb4-cc53-269975830275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv83uWZr8jJn"
      },
      "outputs": [],
      "source": [
        "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context.\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YShG_wuK8k6A"
      },
      "outputs": [],
      "source": [
        "# Creating a spark context class\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNGANBUT8pbJ"
      },
      "outputs": [],
      "source": [
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames Import Student\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"6g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrhn0-B48nVl"
      },
      "source": [
        "## Import Spark Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvVzdajP9-zk",
        "outputId": "f0a2df5b-118e-422e-c9ff-fcee1faa59c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3506a890"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"/content/drive/MyDrive/Financial_Dataset/archive/transactions_data_updated.csv\", header=True, inferSchema=True)\n",
        "#df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W7DdsBG-8Ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc69480-42c6-4c42-b3f1-ec32ed6d2336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- client_id: integer (nullable = true)\n",
            " |-- card_id: integer (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- use_chip: string (nullable = true)\n",
            " |-- merchant_id: integer (nullable = true)\n",
            " |-- merchant_city: string (nullable = true)\n",
            " |-- merchant_state: string (nullable = true)\n",
            " |-- zip: double (nullable = true)\n",
            " |-- mcc: integer (nullable = true)\n",
            " |-- errors: string (nullable = true)\n",
            " |-- is_fraud: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ0k98b2_NCW"
      },
      "source": [
        "### Merge json labels of fraud detection into the csv file (ignore this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG_X2qBY_qLo"
      },
      "outputs": [],
      "source": [
        "#import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_1Zn-RS_1S-"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file as a dictionary\n",
        "'''\n",
        "with open('/content/drive/My Drive/Financial_Dataset/archive/train_fraud_labels.json') as f:\n",
        "    fraud_labels = json.load(f)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dca32de"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import lit, col, create_map\n",
        "from itertools import chain\n",
        "'''\n",
        "# Convert the fraud_labels dictionary to a Spark DataFrame\n",
        "fraud_df = spark.createDataFrame(fraud_labels['target'].items(), [\"id\", \"is_fraud\"])\n",
        "\n",
        "# Cast the 'id' column in fraud_df to integer to match the transaction DataFrame\n",
        "fraud_df = fraud_df.withColumn(\"id\", col(\"id\").cast(\"integer\"))\n",
        "\n",
        "# Join the transaction DataFrame with the fraud_df on the 'id' column (on the right)\n",
        "df = df.join(fraud_df, on=\"id\", how=\"right\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWQEBIOO_lp1"
      },
      "outputs": [],
      "source": [
        "## I think we should convert the amount into float first\n",
        "from pyspark.sql.functions import udf\n",
        "def clean(s):\n",
        "  if s[0] == '$':\n",
        "    s = s[1:]\n",
        "  return s\n",
        "\n",
        "amountToFloatUDF = udf(lambda s:clean(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_rwmlDTMYp8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "df = df.withColumn(\"amount\",amountToFloatUDF(\"amount\").cast(DoubleType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qExgtKFB3mw"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbsJpssuCmKk"
      },
      "outputs": [],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b620uac6HC2e"
      },
      "source": [
        "# Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0546004"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum,count,avg\n",
        "\n",
        "# Group by 'merchant_city' and calculate the sum of 'amount'\n",
        "city_amount_df = df.groupBy(\"merchant_city\").agg(\n",
        "    sum(\"amount\").alias(\"total_amount\"),\n",
        "    count(\"*\").alias(\"entry_count\"),\n",
        "    avg(\"amount\").alias(\"average_amount\")\n",
        ")\n",
        "\n",
        "# Sort the result by 'total_amount' in descending order\n",
        "sorted_city_amount_df = city_amount_df.orderBy(\"total_amount\", ascending=False)\n",
        "sorted_city_amount_df.show(10, truncate=False) ## this is the data frames with the count of all data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43UQt18E0LFY"
      },
      "outputs": [],
      "source": [
        "# Don't use anymore found a more efficient way\n",
        "'''def getYear(date):\n",
        "  columns = date.split(\"-\")\n",
        "  try:\n",
        "    year = int(columns)\n",
        "    if year < 2000 and year < 2025:\n",
        "      return year\n",
        "  except:\n",
        "    pass\n",
        "  return -1\n",
        "getYearUDF = udf(lambda date : getYear(date))\n",
        "#year_amount_df = df.withColumn(\"year\", getYearUDF(\"date\")).groupBy(\"year\").agg(sum(\"amount\").alias(\"total_amount\"))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ou1KV2az7RU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum, year\n",
        "\n",
        "# Instead of using the getYearUDF use the year function in the spark sql liabrary\n",
        "year_amount_df = df.withColumn(\"year\", year(\"date\")).groupBy(\"year\").agg(\n",
        "    sum(\"amount\").alias(\"total_amount\"),\n",
        "    count(\"*\").alias(\"entry_count\"),\n",
        "    avg(\"amount\").alias(\"average_amount\")\n",
        ")\n",
        "\n",
        "# Sort the result by 'year' in ascending order\n",
        "sorted_year_amount_df = year_amount_df.orderBy(\"year\",ascending=False)\n",
        "sorted_year_amount_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s28hXnl3zkTY"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4XJppVQcAHL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goeXQB77j8Vl",
        "outputId": "4a305f86-8c24-464b-a4c6-487771fedc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+---------+-------+-------+------------------+-----------+-------------+--------------+-------+----+------+--------+\n",
            "|     id|               date|client_id|card_id| amount|          use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|is_fraud|\n",
            "+-------+-------------------+---------+-------+-------+------------------+-----------+-------------+--------------+-------+----+------+--------+\n",
            "|7475338|2010-01-01 00:23:00|      554|   3912|  $3.51| Swipe Transaction|      67570|     Pearland|            TX|77581.0|5311|  NULL|      No|\n",
            "|7475353|2010-01-01 00:43:00|      301|   3742| $10.17|Online Transaction|      39021|       ONLINE|          NULL|   NULL|4784|  NULL|      No|\n",
            "|7475365|2010-01-01 01:01:00|      820|    127|$270.22|Online Transaction|      73186|       ONLINE|          NULL|   NULL|4814|  NULL|      No|\n",
            "|7475379|2010-01-01 01:22:00|      986|   4755|  $1.85| Swipe Transaction|      14528|  Garden City|            NY|11530.0|5499|  NULL|      No|\n",
            "|7475400|2010-01-01 01:54:00|      597|   4540| $14.20| Swipe Transaction|      66541|San Francisco|            CA|94131.0|5300|  NULL|      No|\n",
            "|7475417|2010-01-01 02:11:00|       34|   1166|  $4.07| Swipe Transaction|      49789|   Sacramento|            CA|95829.0|5541|  NULL|      No|\n",
            "|7475419|2010-01-01 02:12:00|     1424|   4710|  $2.31| Swipe Transaction|      95811|     Seligman|            AZ|86337.0|5411|  NULL|      No|\n",
            "|7475432|2010-01-01 02:25:00|     1214|   5508| $51.58|Online Transaction|      98436|       ONLINE|          NULL|   NULL|5192|  NULL|      No|\n",
            "|7475464|2010-01-01 03:13:00|      957|   4532| $14.21| Swipe Transaction|      44795|   Marysville|            OH|43040.0|3780|  NULL|      No|\n",
            "|7475495|2010-01-01 04:21:00|     1635|    115| $14.50| Swipe Transaction|      45501|      Jamaica|            NY|11434.0|5912|  NULL|      No|\n",
            "|7475501|2010-01-01 04:31:00|      526|   5917| $22.23|Online Transaction|      16798|       ONLINE|          NULL|   NULL|4121|  NULL|      No|\n",
            "|7475564|2010-01-01 05:56:00|      530|   3344|  $2.98| Swipe Transaction|      88646|       Colton|            CA|92324.0|5812|  NULL|      No|\n",
            "|7475570|2010-01-01 06:00:00|     1080|   2562| $43.13| Swipe Transaction|      11468|        Macon|            GA|31204.0|5970|  NULL|      No|\n",
            "|7475600|2010-01-01 06:08:00|     1381|   3738|  $5.41| Swipe Transaction|      11264|       Mentor|            OH|44060.0|5812|  NULL|      No|\n",
            "|7475604|2010-01-01 06:09:00|     1150|   4672|  $7.09| Swipe Transaction|      19464|        Hurst|            TX|76054.0|5812|  NULL|      No|\n",
            "|7475632|2010-01-01 06:17:00|     1675|   4692|$-86.00| Swipe Transaction|      43293|     Puyallup|            WA|98373.0|5499|  NULL|      No|\n",
            "|7475634|2010-01-01 06:18:00|      571|   5960|  $1.77| Swipe Transaction|      49903|  Martinsburg|            WV|25401.0|5411|  NULL|      No|\n",
            "|7475663|2010-01-01 06:26:00|       64|   5429|$190.91| Swipe Transaction|      14914|        Ocoee|            FL|34761.0|7995|  NULL|      No|\n",
            "|7475699|2010-01-01 06:38:00|     1604|   2188|  $3.66|Online Transaction|      85247|       ONLINE|          NULL|   NULL|5815|  NULL|      No|\n",
            "|7475712|2010-01-01 06:41:00|     1898|   4543| $40.40|Online Transaction|      39021|       ONLINE|          NULL|   NULL|4784|  NULL|      No|\n",
            "+-------+-------------------+---------+-------+-------+------------------+-----------+-------------+--------------+-------+----+------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8yLOobieE4W"
      },
      "source": [
        "### Extract feature from date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9L3_5B3zlfD",
        "outputId": "7e049014-557e-4af0-d60e-51d7c038d648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+---------+-------+-------+--------+-----------+-------------+--------------+-------+----+------+--------+----+---------+\n",
            "|     id|               date|client_id|card_id| amount|use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|is_fraud|hour|dayofweek|\n",
            "+-------+-------------------+---------+-------+-------+--------+-----------+-------------+--------------+-------+----+------+--------+----+---------+\n",
            "|7475338|2010-01-01 00:23:00|      554|   3912|  $3.51|    NULL|      67570|     Pearland|            TX|77581.0|5311|  NULL|      No|   0|        6|\n",
            "|7475353|2010-01-01 00:43:00|      301|   3742| $10.17|    NULL|      39021|       ONLINE|          NULL|   NULL|4784|  NULL|      No|   0|        6|\n",
            "|7475365|2010-01-01 01:01:00|      820|    127|$270.22|    NULL|      73186|       ONLINE|          NULL|   NULL|4814|  NULL|      No|   1|        6|\n",
            "|7475379|2010-01-01 01:22:00|      986|   4755|  $1.85|    NULL|      14528|  Garden City|            NY|11530.0|5499|  NULL|      No|   1|        6|\n",
            "|7475400|2010-01-01 01:54:00|      597|   4540| $14.20|    NULL|      66541|San Francisco|            CA|94131.0|5300|  NULL|      No|   1|        6|\n",
            "|7475417|2010-01-01 02:11:00|       34|   1166|  $4.07|    NULL|      49789|   Sacramento|            CA|95829.0|5541|  NULL|      No|   2|        6|\n",
            "|7475419|2010-01-01 02:12:00|     1424|   4710|  $2.31|    NULL|      95811|     Seligman|            AZ|86337.0|5411|  NULL|      No|   2|        6|\n",
            "|7475432|2010-01-01 02:25:00|     1214|   5508| $51.58|    NULL|      98436|       ONLINE|          NULL|   NULL|5192|  NULL|      No|   2|        6|\n",
            "|7475464|2010-01-01 03:13:00|      957|   4532| $14.21|    NULL|      44795|   Marysville|            OH|43040.0|3780|  NULL|      No|   3|        6|\n",
            "|7475495|2010-01-01 04:21:00|     1635|    115| $14.50|    NULL|      45501|      Jamaica|            NY|11434.0|5912|  NULL|      No|   4|        6|\n",
            "|7475501|2010-01-01 04:31:00|      526|   5917| $22.23|    NULL|      16798|       ONLINE|          NULL|   NULL|4121|  NULL|      No|   4|        6|\n",
            "|7475564|2010-01-01 05:56:00|      530|   3344|  $2.98|    NULL|      88646|       Colton|            CA|92324.0|5812|  NULL|      No|   5|        6|\n",
            "|7475570|2010-01-01 06:00:00|     1080|   2562| $43.13|    NULL|      11468|        Macon|            GA|31204.0|5970|  NULL|      No|   6|        6|\n",
            "|7475600|2010-01-01 06:08:00|     1381|   3738|  $5.41|    NULL|      11264|       Mentor|            OH|44060.0|5812|  NULL|      No|   6|        6|\n",
            "|7475604|2010-01-01 06:09:00|     1150|   4672|  $7.09|    NULL|      19464|        Hurst|            TX|76054.0|5812|  NULL|      No|   6|        6|\n",
            "|7475632|2010-01-01 06:17:00|     1675|   4692|$-86.00|    NULL|      43293|     Puyallup|            WA|98373.0|5499|  NULL|      No|   6|        6|\n",
            "|7475634|2010-01-01 06:18:00|      571|   5960|  $1.77|    NULL|      49903|  Martinsburg|            WV|25401.0|5411|  NULL|      No|   6|        6|\n",
            "|7475663|2010-01-01 06:26:00|       64|   5429|$190.91|    NULL|      14914|        Ocoee|            FL|34761.0|7995|  NULL|      No|   6|        6|\n",
            "|7475699|2010-01-01 06:38:00|     1604|   2188|  $3.66|    NULL|      85247|       ONLINE|          NULL|   NULL|5815|  NULL|      No|   6|        6|\n",
            "|7475712|2010-01-01 06:41:00|     1898|   4543| $40.40|    NULL|      39021|       ONLINE|          NULL|   NULL|4784|  NULL|      No|   6|        6|\n",
            "+-------+-------------------+---------+-------+-------+--------+-----------+-------------+--------------+-------+----+------+--------+----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import hour, dayofweek, col\n",
        "\n",
        "df = df.withColumn(\"hour\", hour(\"date\"))\n",
        "df = df.withColumn(\"dayofweek\", dayofweek(\"date\"))\n",
        "df = df.withColumn(\"use_chip\", col(\"use_chip\").cast(\"int\"))\n",
        "df.show(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFJ5yytoeHHm"
      },
      "source": [
        "### Remove unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ryhk1nQ8eKrd"
      },
      "outputs": [],
      "source": [
        "df = df.drop(\"id\", \"date\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0v3_EAUkM70"
      },
      "source": [
        "### Clean \\$ sign from the amount column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-pUgSggkUC2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import regexp_replace, col\n",
        "df = df.withColumn(\"amount\", regexp_replace(\"amount\", \"\\\\$\", \"\").cast(\"float\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hIDLEks-ED3"
      },
      "source": [
        "### Split just a fraction of data for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja7e4frB-JiM"
      },
      "outputs": [],
      "source": [
        "df = df.sample(withReplacement=False, fraction=0.005, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGknLuBv-SIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e373e73d-57df-481a-bd87-78d0821c8667"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44435"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgH8vB_beNYP"
      },
      "source": [
        "### Encoding categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nx9CXy4eWSN"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "\n",
        "categorical_cols = [\"client_id\", \"card_id\", \"merchant_id\", \"merchant_city\", \"merchant_state\", \"zip\", \"mcc\"]\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\") for col in categorical_cols]\n",
        "encoders = [OneHotEncoder(inputCol=col+\"_idx\", outputCol=col+\"_vec\") for col in categorical_cols]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX652yWL3KNY",
        "outputId": "b4a0e882-b4b6-47f7-b9bc-6185f61d02da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing: 1/7\n",
            "Indexing: 2/7\n",
            "Indexing: 3/7\n",
            "Indexing: 4/7\n",
            "Indexing: 5/7\n",
            "Indexing: 6/7\n",
            "Indexing: 7/7\n"
          ]
        }
      ],
      "source": [
        "indexed_df = df\n",
        "for i in range(len(indexers)):\n",
        "    indexer = indexers[i]\n",
        "    print(f\"Indexing: {i+1}/{len(indexers)}\")\n",
        "    indexed_df = indexer.fit(indexed_df).transform(indexed_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_df = indexed_df\n",
        "for i in range(len(encoders)):\n",
        "  encoder = encoders[i]\n",
        "  print(f\"Encoding: {i+1}/{len(encoders)}\")\n",
        "  encoder_df = encoder.fit(encoder_df).transform(encoder_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzdbcj5BAZR4",
        "outputId": "33989d0e-d800-43ea-d368-0bb58db5d9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding: 1/7\n",
            "Encoding: 2/7\n",
            "Encoding: 3/7\n",
            "Encoding: 4/7\n",
            "Encoding: 5/7\n",
            "Encoding: 6/7\n",
            "Encoding: 7/7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGJHH0aa3aTT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzKGq75k3Zbl"
      },
      "outputs": [],
      "source": [
        "# from pyspark.ml.feature import VectorAssembler\n",
        "# from pyspark.sql.functions import col\n",
        "\n",
        "# # Define the feature columns\n",
        "# included_columns = [\"merchant_city\"]\n",
        "# indexed_columns = [column+\"_idx\" for column in included_columns]\n",
        "# featuresColumns = indexed_columns + [\"mcc\",\"amount\"]\n",
        "# indexed_df_filled = indexed_df.na.fill(0, subset=featuresColumns)\n",
        "\n",
        "# assembler = VectorAssembler(inputCols=featuresColumns, outputCol=\"features\")\n",
        "# assembled_df = assembler.transform(indexed_df_filled)\n",
        "\n",
        "# # Show the schema of the assembled dataframe\n",
        "# assembled_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAm-wsAqeo6F"
      },
      "source": [
        "### Assembling features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47-yYbl2etDx"
      },
      "outputs": [],
      "source": [
        "feature_cols = [\"hour\", \"dayofweek\"] + [col+\"_vec\" for col in categorical_cols]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply vector assembler to encoder_df\n",
        "assembled_df = assembler.transform(encoder_df)"
      ],
      "metadata": {
        "id": "ASLD1Hp1EyIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembled_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l6RpcBTFIPP",
        "outputId": "a99e2aa0-c543-44b8-e3ed-e12b4b4086fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+------+--------+-----------+-------------+--------------+-------+----+------+--------+----+---------+-------------+-----------+---------------+-----------------+------------------+-------+-------+------------------+-------------------+------------------+------------------+------------------+-------------------+----------------+--------------------+\n",
            "|client_id|card_id|amount|use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|is_fraud|hour|dayofweek|client_id_idx|card_id_idx|merchant_id_idx|merchant_city_idx|merchant_state_idx|zip_idx|mcc_idx|     client_id_vec|        card_id_vec|   merchant_id_vec| merchant_city_vec|merchant_state_vec|            zip_vec|         mcc_vec|            features|\n",
            "+---------+-------+------+--------+-----------+-------------+--------------+-------+----+------+--------+----+---------+-------------+-----------+---------------+-----------------+------------------+-------+-------+------------------+-------------------+------------------+------------------+------------------+-------------------+----------------+--------------------+\n",
            "|     1686|   2999|173.75|    NULL|      10388|    Lafayette|            CO|80026.0|5300|  NULL|      No|  15|        7|         97.0|      424.0|          242.0|            199.0|              22.0|  106.0|    6.0| (1219,[97],[1.0])| (3870,[424],[1.0])|(6741,[242],[1.0])|(3627,[199],[1.0])|   (96,[22],[1.0])| (6410,[106],[1.0])| (107,[6],[1.0])|(22072,[0,1,99,16...|\n",
            "|      754|   5450| 12.72|    NULL|      64032| Myrtle Beach|            SC|29579.0|5912|  NULL|      No|   6|        1|        123.0|      398.0|          206.0|            103.0|              19.0|   21.0|    4.0|(1219,[123],[1.0])| (3870,[398],[1.0])|(6741,[206],[1.0])|(3627,[103],[1.0])|   (96,[19],[1.0])|  (6410,[21],[1.0])| (107,[4],[1.0])|(22072,[0,1,125,1...|\n",
            "|     1677|   4969| 10.76|    NULL|      83480|      Lubbock|            TX|79414.0|9402|  NULL|      No|  16|        6|        198.0|       51.0|           18.0|             60.0|               1.0|  116.0|   21.0|(1219,[198],[1.0])|  (3870,[51],[1.0])| (6741,[18],[1.0])| (3627,[60],[1.0])|    (96,[1],[1.0])| (6410,[116],[1.0])|(107,[21],[1.0])|(22072,[0,1,200,1...|\n",
            "|     1008|   5850| 12.53|    NULL|      16798|       ONLINE|          NULL|   NULL|4121|  NULL|      No|  16|        6|        275.0|     1635.0|           15.0|              0.0|              96.0| 6410.0|    9.0|(1219,[275],[1.0])|(3870,[1635],[1.0])| (6741,[15],[1.0])|  (3627,[0],[1.0])|        (96,[],[])|       (6410,[],[])| (107,[9],[1.0])|(22072,[0,1,277,2...|\n",
            "|     1576|   5821| 23.92|    NULL|      50783|   Farmington|            MI|48335.0|5411|  NULL|      No|   8|        1|        315.0|     3192.0|            7.0|             42.0|               8.0| 2428.0|    0.0|(1219,[315],[1.0])|(3870,[3192],[1.0])|  (6741,[7],[1.0])| (3627,[42],[1.0])|    (96,[8],[1.0])|(6410,[2428],[1.0])| (107,[0],[1.0])|(22072,[0,1,317,4...|\n",
            "+---------+-------+------+--------+-----------+-------------+--------------+-------+----+------+--------+----+---------+-------------+-----------+---------------+-----------------+------------------+-------+-------+------------------+-------------------+------------------+------------------+------------------+-------------------+----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1nJajUTe8Zq"
      },
      "source": [
        "### Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPlBO1LXe-4C"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "rf = RandomForestRegressor(labelCol=\"amount\", featuresCol=\"features\", numTrees=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = assembled_df.randomSplit([0.8, 0.2], seed=12345678)"
      ],
      "metadata": {
        "id": "TbWiPzBLFuUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = rf.fit(train)"
      ],
      "metadata": {
        "id": "-lg4WZyoEt6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model_regression\")"
      ],
      "metadata": {
        "id": "v2F67S_9YUjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZvYz2dDfJuD"
      },
      "source": [
        "### Spark pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4HvCtuMfQyu"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler, rf])\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=12345678)\n",
        "model = pipeline.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfSb4NldfpmJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdb6stS1fehI"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8YguWBAfgg9"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(test)\n",
        "predictions.select(\"amount\", \"prediction\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHR1cGOU20hm"
      },
      "outputs": [],
      "source": [
        "model.save(\"regressionModel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3erZ_9mh6xuf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "for i in range(3600):\n",
        "  print(i)\n",
        "  # Keep colab alive\n",
        "  time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCwASagWxt5v"
      },
      "source": [
        "# Classification\n",
        "- Binary Classification (Logistic Regression) , to classify if the transcation is a fraud or not based on specified features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYwT0BcH-vjS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import DataFrameNaFunctions\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression,DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgdX8vBZ-xPB"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "#do string indexer first, for features like use_chip, merchant_city, errors\n",
        "included_columns = [\"use_chip\", \"merchant_city\", \"errors\"]\n",
        "indexed_columns = [column+\"_indexed\" for column in included_columns]\n",
        "\n",
        "indexers = [StringIndexer(inputCol=included_columns[i], outputCol=indexed_columns[i]).setHandleInvalid(\"keep\")for i in range(len(included_columns))]\n",
        "\n",
        "# The indexers list now contains the StringIndexer stages for each categorical column.\n",
        "# These will be used later, for example, in a Pipeline or applied sequentially.\n",
        "# Apply indexers sequentially to create indexed columns\n",
        "indexed_df = df\n",
        "for indexer in indexers:\n",
        "    indexed_df = indexer.fit(indexed_df).transform(indexed_df)\n",
        "\n",
        "# Show the schema to confirm the new indexed columns exist\n",
        "indexed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InhQL0i7-z6w"
      },
      "outputs": [],
      "source": [
        "# Also do indexer for is_fraud (cuz its a string), which is a label since we want to predict fraud\n",
        "label_indexer = StringIndexer(inputCol=\"is_fraud\", outputCol=\"label\")\n",
        "indexed_df = label_indexer.fit(indexed_df).transform(indexed_df)\n",
        "indexed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv5aFdHZ-3dK"
      },
      "outputs": [],
      "source": [
        "# feature columns for classification (use_chip, merchant_city, errors, mcc, amount)\n",
        "featuresColumns = indexed_columns + [\"mcc\",\"amount\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgxXmOis-5AH"
      },
      "outputs": [],
      "source": [
        "# I decided to use pipeline\n",
        "assembler = VectorAssembler(inputCols=featuresColumns, outputCol=\"features\")\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"scaled_features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MXayw_y-6m_"
      },
      "outputs": [],
      "source": [
        "# Creating and fitting pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, scaler, lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGcdQTa--8KP"
      },
      "outputs": [],
      "source": [
        "(trainingData, testData) = indexed_df.randomSplit([0.8,0.2], seed = 6580043)\n",
        "model = pipeline.fit(trainingData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8oRmAA--_bW"
      },
      "source": [
        "## Hyperparameter Tuning (Model Tuning)\n",
        "improve the performance of the algorithm on that data by trying different settings and pick the one that performs best using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLCwkFd-_ANo"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.fitIntercept, [False, True]) \\\n",
        "    .addGrid(lr.maxIter, [5, 10,20]) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxS7erhl_DtH"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=MulticlassClassificationEvaluator(),\n",
        "                          numFolds=3)  # use 3+ folds in practice\n",
        "\n",
        "# Run cross-validation, and choose the best set of parameters.\n",
        "cvModel = crossval.fit(indexed_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqwhCADd_FZn"
      },
      "outputs": [],
      "source": [
        "# list of average evaluation scores (e.g., accuracy) for each hyperparameter combination tested\n",
        "cvModel.avgMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "potWrXQZ_G9V"
      },
      "outputs": [],
      "source": [
        "predictions = cvModel.transform(testData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFBwBICQ_HSu"
      },
      "outputs": [],
      "source": [
        "predictions.select(\"probability\",\"prediction\",\"label\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPClnx_T_MiU"
      },
      "source": [
        "## Evalutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSPnDTKB_MNG"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLQblLv6_O2c"
      },
      "outputs": [],
      "source": [
        "# Evaluate model performance\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy =\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6tBPs3B_QUW"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = evaluator.evaluate(predictions)\n",
        "print(\"Recall =\", recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCk5z0lH_R-_"
      },
      "outputs": [],
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator.evaluate(predictions)\n",
        "print(\"F1 score = \", f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN-1qcNZ_UBd"
      },
      "outputs": [],
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"label\")\n",
        "# true positive rate vs false positive\n",
        "AUC = evaluator.evaluate(predictions)\n",
        "print(\"AUC = %g \" % (AUC))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xbpneH7_V7D"
      },
      "outputs": [],
      "source": [
        "predictions.select(\"probability\",\"prediction\", \"label\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ITOYpTr_YFU"
      },
      "outputs": [],
      "source": [
        "columns_to_save = [\"id\", \"prediction\", \"is_fraud\"]\n",
        "predictions.select(columns_to_save).write.save(\"classification_predictions\", format=\"com.databricks.spark.csv\", header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_1RKLId_c8i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "files = glob.glob(\"classification_predictions/part-*\")\n",
        "df = pd.concat([pd.read_csv(f) for f in files])\n",
        "df.to_csv(\"merged_predictions.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeLkZ936TaDJ"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MJSog5lVtxQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "csvfile = './classification_predictions.csv'\n",
        "test_df = pd.read_csv(csvfile)\n",
        "\n",
        "result=[]\n",
        "actual=[]\n",
        "\n",
        "#columns : id, predicted, actual\n",
        "\n",
        "# Use the DataFrame directly instead of reading the file line by line\n",
        "# 0 = no fraud, 1 = fraud\n",
        "for index, row in test_df.iterrows():\n",
        "    result.append(row['prediction'])\n",
        "    if row['is_fraud'] == 'Yes':\n",
        "        actual.append(1)\n",
        "    else:\n",
        "        actual.append(0)\n",
        "\n",
        "cnf_mat=confusion_matrix(actual,result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kpXh3l1XLrf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(cnf_mat)\n",
        "print('Test Accuracy:', accuracy_score(actual,result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl1S5pI9awvC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "classes = [\"0\", \"1\"]\n",
        "df_cfm = pd.DataFrame(cnf_mat, index = classes, columns = classes)\n",
        "plt.figure(figsize = (10,7))\n",
        "\n",
        "cfm_plot = sn.heatmap(df_cfm, annot=True)\n",
        "cfm_plot.figure.savefig(\"cfm.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNiJWidvdweC"
      },
      "outputs": [],
      "source": [
        "# initially i accidentally swapped the false neg and true pos so the matrix looked odd. but i now changed them to the correct position, sorry\n",
        "group_names = [\"True Neg\",\"False Pos\",\"False Neg\", \"True Pos\"]\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_mat.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in cnf_mat.flatten()/np.sum(cnf_mat)]\n",
        "\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sn.heatmap(cnf_mat, annot=labels, fmt='', cmap='Blues')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVsV6ZwVeet4"
      },
      "source": [
        "# Clusterring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jLeyI5Wem3e"
      },
      "outputs": [],
      "source": [
        "df.describe().toPandas().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT409E3V5IPU"
      },
      "source": [
        "### Next step is choosing relevant data to include as a features column\n",
        "\n",
        "Id does not have any meaning so, id, client_id, card_id, merchant_id will be exclude\n",
        "\n",
        "But before we can use the column to analyze we have to use the indexer to change all the string value to numerical value before using it to do the analysis or training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyAmLio65r0P"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "included_columns = [\"use_chip\", \"merchant_city\", \"errors\", \"is_fraud\"]\n",
        "indexed_columns = [column+\"_indexed\" for column in included_columns]\n",
        "\n",
        "indexers = [StringIndexer(inputCol=included_columns[i], outputCol=indexed_columns[i]).setHandleInvalid(\"keep\")for i in range(len(included_columns))]\n",
        "\n",
        "# The indexers list now contains the StringIndexer stages for each categorical column.\n",
        "# These will be used later, for example, in a Pipeline or applied sequentially.\n",
        "# Apply indexers sequentially to create indexed columns\n",
        "indexed_df = df\n",
        "for indexer in indexers:\n",
        "    indexed_df = indexer.fit(indexed_df).transform(indexed_df)\n",
        "\n",
        "# Show the schema to confirm the new indexed columns exist\n",
        "indexed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNCQEMupAI3S"
      },
      "outputs": [],
      "source": [
        "print(indexed_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3xG3898ezEX"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Define the feature columns\n",
        "featuresColumns = indexed_columns + [\"mcc\",\"amount\"]\n",
        "indexed_df_filled = indexed_df.na.fill(0, subset=featuresColumns)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=featuresColumns, outputCol=\"features\")\n",
        "assembled_df = assembler.transform(indexed_df_filled)\n",
        "\n",
        "# Show the schema of the assembled dataframe\n",
        "assembled_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3F0CCCbB1WD"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "scaler = StandardScaler(inputCol = \"features\", outputCol = \"scaled_features\")\n",
        "scaled_df = scaler.fit(assembled_df).transform(assembled_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9CMtmIc9Y6B"
      },
      "outputs": [],
      "source": [
        "# Test data, just for debuggin code and don't have to wait 5mins\n",
        "(trainData,testData) = scaled_df.randomSplit([0.7,0.3], seed = 6580081)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO44oTd8D0NK"
      },
      "outputs": [],
      "source": [
        "# Increase parallelism, making the job faster and more memory-efficient\n",
        "trainData = trainData.repartition(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF-EGNnJuZQo"
      },
      "source": [
        "## KMeans\n",
        "K-means is an iterative, centroid-based clustering algorithm that partitions a dataset into similar groups based on the distance between their centroids. Default k is 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8LMzj4gCeTP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Now fit KMeans on the processed trainData using the existing 'scaled_features'\n",
        "kmeans = KMeans(featuresCol='scaled_features')\n",
        "model = kmeans.fit(trainData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaZKxKr6946T"
      },
      "outputs": [],
      "source": [
        "predictions = model.transform(testData)\n",
        "predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBirFXQLMCed"
      },
      "outputs": [],
      "source": [
        "predictions.groupBy('prediction').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH_CUtC3wRR3"
      },
      "source": [
        "## elbow method to determine the best k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKigySkHwby1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cost = []\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(featuresCol='scaled_features', k=k, maxIter=10)\n",
        "    model = kmeans.fit(trainData)\n",
        "    cost.append(model.summary.trainingCost)\n",
        "\n",
        "plt.plot(range(2, 11), cost, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Within Set Sum of Squared Errors (WSS)')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F2Gdx50S9Ct"
      },
      "source": [
        "We can see that the graph's steep drops from k = 2 to k = 5\n",
        "And after k = 5, the reduction slows down (means small improvements for more clusters), not worth it. So k should be 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wdzKu-1UETS"
      },
      "outputs": [],
      "source": [
        "# clusters = 5\n",
        "kmeans = KMeans(featuresCol='scaled_features', k=5)\n",
        "model = kmeans.fit(trainData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elGlRmwMVLgY"
      },
      "outputs": [],
      "source": [
        "# make predictions\n",
        "predictions = model.transform(trainData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcnLfyTzVbF2"
      },
      "outputs": [],
      "source": [
        "predictions.groupBy('prediction').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh75DXdpdDyf"
      },
      "source": [
        "Cost (based on Silhouette Score)\n",
        "- Close to 1 â†’ well-separated clusters\n",
        "- Close to 0 â†’ overlapping clusters, bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNysrMzNcyoS"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "cost = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYBx5V6Gc2IB"
      },
      "outputs": [],
      "source": [
        "print(cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j7IO_p7c4lR"
      },
      "outputs": [],
      "source": [
        "centers = model.clusterCenters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guKN2_P6c6ah"
      },
      "outputs": [],
      "source": [
        "print(centers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yXgWm54k0qx"
      },
      "outputs": [],
      "source": [
        "featuresColumns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2x3w9DaooEt"
      },
      "outputs": [],
      "source": [
        "# get stats for each cluster\n",
        "for i in range(5):\n",
        "  predictions.filter(predictions['prediction'] == i) \\\n",
        "        .select(featuresColumns) \\\n",
        "        .describe() \\\n",
        "        .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozO6zqS07aGy"
      },
      "outputs": [],
      "source": [
        "# prediction is cluster number\n",
        "columns_to_save = [\"id\", \"prediction\"]\n",
        "predictions.select(columns_to_save).write.save(\"clustering_predictions\", format=\"com.databricks.spark.csv\", header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRM-Qmcl9OwI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "files = glob.glob(\"clustering_predictions/part-*\")\n",
        "df = pd.concat([pd.read_csv(f) for f in files])\n",
        "df.to_csv(\"merged_predictions.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WgwDMypD1Hu"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09c5709d"
      },
      "outputs": [],
      "source": [
        "# Merge the two dataframes based on the 'id' column\n",
        "merged_df = df1.join(df2, on=\"id\", how=\"inner\")\n",
        "\n",
        "# Show the first few rows of the merged dataframe to verify\n",
        "merged_df.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b620uac6HC2e"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}